{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58f4c5c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'brave-watch-414204'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project = !gcloud config get-value project\n",
    "PROJECT_ID = project[0]\n",
    "PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bac4cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16cd0912",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1/3085960609.py:3: DeprecationWarning: The module `kfp.v2` is deprecated and will be removed in a futureversion. Please import directly from the `kfp` namespace, instead of `kfp.v2`.\n",
      "  from kfp.v2 import dsl\n"
     ]
    }
   ],
   "source": [
    "import kfp\n",
    "\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import pipeline\n",
    "from kfp.v2.dsl import component\n",
    "from kfp.v2.dsl import OutputPath\n",
    "from kfp.v2.dsl import InputPath\n",
    "\n",
    "\n",
    "from kfp.v2.dsl import Output\n",
    "from kfp.v2.dsl import Metrics\n",
    "\n",
    "from kfp.v2 import compiler\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "\n",
    "from kfp.v2.dsl import (Artifact,\n",
    "                        Dataset,\n",
    "                        Input,\n",
    "                        Model,\n",
    "                        Output,\n",
    "                        Metrics,\n",
    "                        ClassificationMetrics,\n",
    "                        component,\n",
    "                        Markdown)\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.aiplatform import pipeline_jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f01ef0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Kubeflow Pipeline (KFP) Definition\n",
    "\n",
    "##### Kubeflow is an open-source platform designed to make it easy to deploy, manage, and scale machine learning models on Kubernetes.\n",
    "\n",
    "##### Component-based Architecture: Kubeflow is built using a modular architecture, allowing users to pick and choose components based on their requirements.\n",
    "\n",
    "##### [Click here to see how to construct a Kubeflow Pipeline:](https://www.youtube.com/watch?v=gtVHw5YCRhE&ab_channel=MLEngineer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cf023c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Kubeflow Pyspark Component VS Big Query\n",
    "##### [Click here to see how to construct a Pyspark Servless component:](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage3/get_started_with_dataproc_serverless_pipeline_components.ipynb)\n",
    "\n",
    "##### Spark allow us to directly read csv from bucket but Big Query doesn't support his function\n",
    "##### Big Query is easier to config from the UI\n",
    "\n",
    "##### Spark has a series of Libs for scalability(SparkML ect...)\n",
    "##### Pandas and sk learn may not avoidable by using Big Query -> (\"Upload file to big query\", No similar thing like Spark ML)\n",
    "\n",
    "#### Utilize Big Query require to redesign the pipeline trigger flow \n",
    "##### With the current design(triiger by file upload) 1. upload file -> 2.write data to bigquery -> 3. excute training pipline \n",
    "##### The new way to triiger: when new data entery comes into Big Query Table \n",
    "##### [Click here to see how to trigger cloud function with Big Query:](https://medium.com/@sidspwc/trigger-event-processing-from-big-query-using-gcp-eventarc-and-cloud-functions-c1c420b3e0df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc293ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get Data Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "545334a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\"numpy\",\"pandas\",\"fsspec\",\"db-dtypes\", \"google-cloud-bigquery\"]\n",
    ")\n",
    "def get_data(\n",
    "    # project_id: Input[str],\n",
    "    dataset: Output[Dataset],\n",
    "):\n",
    "    from google.cloud import bigquery\n",
    "    \n",
    "    # print(\"Project ID >>>>>>\" + project_id)\n",
    "    \n",
    "    # Initialize BigQuery client\n",
    "    client = bigquery.Client(project='brave-watch-414204')\n",
    "    \n",
    "    sql_query = \"\"\"\n",
    "    SELECT user_id, recipe_id, rating \n",
    "    FROM `brave-watch-414204.RecipeQuery.interactions`\n",
    "    \"\"\"\n",
    "    \n",
    "    # Execute the query job\n",
    "    query_job = client.query(sql_query)\n",
    "\n",
    "    # Convert the query resutlt to dataframe\n",
    "    data = query_job.to_dataframe()\n",
    "    \n",
    "    # filter out target person\n",
    "    # filtered_df = data.groupby('user_id').filter(lambda x: len(x) > 20 and len(x) < 150)\n",
    "    \n",
    "    # write the model out to the trained_model\n",
    "    # with open(trained_model, \"wb\") as f:\n",
    "    #     pickle.dump(model_svm, f)\n",
    "\n",
    "    data.to_csv(dataset.path, header=True, index = False)\n",
    "    # filtered_df.to_csv(dataset.path, header=True, index = False)\n",
    "    # print(\"Dataset path >>>>>>\" + dataset.path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faa5712",
   "metadata": {},
   "source": [
    "### Feature Engineering Component"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bbb6b24f",
   "metadata": {},
   "source": [
    "################################################################\n",
    "\n",
    "Our Feature Engineer Component Here\n",
    "\n",
    "################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770f9dc5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3209c800",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9\", \n",
    "    packages_to_install=[\"numpy\",\"pandas\",\"scikit-surprise\",\"fsspec\",\"gcsfs\",\"google-cloud-storage\"]\n",
    ")\n",
    "def training_model(\n",
    "    dataset: Input[Dataset],\n",
    "    training_status: Output[bool]\n",
    "):\n",
    "    import pickle\n",
    "    import pandas as pd\n",
    "    from google.cloud import storage\n",
    "\n",
    "    import surprise\n",
    "    # from surprise import SVD\n",
    "    from surprise import KNNBasic\n",
    "    # from surprise import KNNWithMeans\n",
    "    from surprise import Dataset\n",
    "    from surprise import accuracy\n",
    "    from surprise import Reader\n",
    "    from surprise.model_selection import train_test_split\n",
    "    from surprise.model_selection import cross_validate\n",
    "\n",
    "    \n",
    "    print(\"Dataset path >>>>>>\" + dataset.path)\n",
    "    ratings_df = pd.read_csv(dataset.path)\n",
    "    \n",
    "    reader = Reader(rating_scale=(0,6))\n",
    "    data = Dataset.load_from_df(ratings_df, reader)  # assumes dataframe contains: user, item, ratings (in this order)\n",
    "    \n",
    "    # training\n",
    "    # trainset, testset  = train_test_split(data, test_size=0.1)  # select 10% of rating events\n",
    "    trainset = data.build_full_trainset()\n",
    "    # algo = SVD(n_factors = 50)\n",
    "    algo = KNNBasic(k=5,sim_options={'name': 'msd', 'user_based': True }) # User-based CF\n",
    "    # algo = KNNBasic(k=40, sim_options={'name': 'cosine', 'user_based': True}) # ZeroDivisionError: float division WHY !!! \n",
    "    # algo = KNNBasic(k=40, sim_options={'name': 'pearson', 'user_based': True})\n",
    "    # algo = KNNBasic(k=40, sim_options={'name': 'pearson_baseline', 'user_based': True})\n",
    "    algo.fit(trainset)\n",
    "    \n",
    "    # Dump model pickle file\n",
    "    model_bytes = pickle.dumps(algo)\n",
    "    \n",
    "    # Upload the model pickle file to GCS\n",
    "    bucket_name = \"brave-watch-414204\"\n",
    "    # pickle_file_name = \"svd_model.pkl\"\n",
    "    pickle_file_name = \"knn_model.pkl\"\n",
    "    folder_path = \"Model\"\n",
    "    destination_blob_name = f'{folder_path}/{pickle_file_name}'\n",
    "    \n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    blob.upload_from_string(model_bytes)\n",
    "    training_status = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647c4ba7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Construct Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "983d4bf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=\"training_pipeline\",\n",
    "          pipeline_root=\"gs://brave-watch-414204\" + \"training_pipeline\")\n",
    "# def basic_pipeline(project_id: str):\n",
    "#     data_processing = get_data(project_id = project_id)\n",
    "def basic_pipeline():\n",
    "    data_processing = get_data()\n",
    "    training = training_model(dataset = data_processing.outputs['dataset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf966ffc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=basic_pipeline, \n",
    "    package_path=\"training_pipeline.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abf1bcad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "job = pipeline_jobs.PipelineJob(\n",
    "    display_name=\"training_pipeline\",\n",
    "    template_path=\"training_pipeline.yaml\",\n",
    "    enable_caching=False,\n",
    "    location=\"asia-southeast1\",\n",
    "    # parameter_values={\n",
    "    #     'project_id': PROJECT_ID\n",
    "    # }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18cd0de4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/564706877122/locations/asia-southeast1/pipelineJobs/training-pipeline-20240415031105\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/564706877122/locations/asia-southeast1/pipelineJobs/training-pipeline-20240415031105')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/asia-southeast1/pipelines/runs/training-pipeline-20240415031105?project=564706877122\n",
      "PipelineJob projects/564706877122/locations/asia-southeast1/pipelineJobs/training-pipeline-20240415031105 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/564706877122/locations/asia-southeast1/pipelineJobs/training-pipeline-20240415031105 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/564706877122/locations/asia-southeast1/pipelineJobs/training-pipeline-20240415031105 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/564706877122/locations/asia-southeast1/pipelineJobs/training-pipeline-20240415031105 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/564706877122/locations/asia-southeast1/pipelineJobs/training-pipeline-20240415031105 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "job.run(sync=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884965b7",
   "metadata": {},
   "source": [
    "### Save pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9d5b2da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "# Upload the model pickle file to GCS\n",
    "bucket_name = \"brave-watch-414204\"\n",
    "file_name = \"training_pipeline.yaml\"\n",
    "folder_path = \"Pipeline_ymal\"\n",
    "destination_blob_name = f'{folder_path}/{file_name}'\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "blob = bucket.blob(destination_blob_name)\n",
    "blob.upload_from_filename('training_pipeline.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097e6dd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m119",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m119"
  },
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
