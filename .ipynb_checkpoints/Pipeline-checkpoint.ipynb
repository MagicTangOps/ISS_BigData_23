{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87b1f7ad-e3af-4534-ad89-2936894cbf37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'brave-watch-414204'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project = !gcloud config get-value project\n",
    "PROJECT_ID = project[0]\n",
    "PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e58ed5e-d787-4656-bc62-4b7741684fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b98c9af0-5243-41f7-a425-662a1b0c2f75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1/745685549.py:3: DeprecationWarning: The module `kfp.v2` is deprecated and will be removed in a futureversion. Please import directly from the `kfp` namespace, instead of `kfp.v2`.\n",
      "  from kfp.v2 import dsl\n"
     ]
    }
   ],
   "source": [
    "import kfp\n",
    "\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import pipeline\n",
    "from kfp.v2.dsl import component\n",
    "from kfp.v2.dsl import OutputPath\n",
    "from kfp.v2.dsl import InputPath\n",
    "\n",
    "\n",
    "from kfp.v2.dsl import Output\n",
    "from kfp.v2.dsl import Metrics\n",
    "\n",
    "from kfp.v2 import compiler\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "\n",
    "from kfp.v2.dsl import (Artifact,\n",
    "                        Dataset,\n",
    "                        Input,\n",
    "                        Model,\n",
    "                        Output,\n",
    "                        Metrics,\n",
    "                        ClassificationMetrics,\n",
    "                        component,\n",
    "                        Markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a01fcaf-783c-49a7-b196-3ca714529f4d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Kubeflow Pipeline (KFP) Definition\n",
    "\n",
    "##### Kubeflow is an open-source platform designed to make it easy to deploy, manage, and scale machine learning models on Kubernetes.\n",
    "\n",
    "##### Component-based Architecture: Kubeflow is built using a modular architecture, allowing users to pick and choose components based on their requirements.\n",
    "\n",
    "##### [Click here to see how to construct a Kubeflow Pipeline:](https://www.youtube.com/watch?v=gtVHw5YCRhE&ab_channel=MLEngineer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372e4580-1115-422c-8017-f4c010f58535",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Kubeflow Pyspark Component VS Big Query\n",
    "\n",
    "##### Spark allow us to directly read csv from bucket but Big Query doesn't support his function\n",
    "##### Big Query is easier to config from the UI\n",
    "\n",
    "##### Spark has a series of Libs for scalability(SparkML ect...)\n",
    "##### Pandas and sk learn may not avoidable by using Big Query -> (\"Upload file to big query\", No similar thing like Spark ML)\n",
    "\n",
    "#### Need to redesign the pipeline trigger flow \n",
    "##### With the same design(triiger by file upload) 1. upload file -> 2.write data to bigquery -> 3. excute training pipline \n",
    "##### The new way to triiger: when new data entery comes into Big Query Table \n",
    "##### [Click here to see how to trigger cloud function with Big Query:](https://medium.com/@sidspwc/trigger-event-processing-from-big-query-using-gcp-eventarc-and-cloud-functions-c1c420b3e0df)\n",
    "\n",
    "##### [Click here to see how to construct a Pyspark Servless component:](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage3/get_started_with_dataproc_serverless_pipeline_components.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c888ac75-af1f-44b4-8399-2a85fc73bc92",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get Data Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f466764-1e64-48ca-a8d9-d0fbbb291d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\"numpy\",\"pandas\",\"fsspec\",\"gcsfs\",\"pyspark\"]\n",
    ")\n",
    "def get_data(\n",
    "    dataset: Output[Dataset],\n",
    "):\n",
    "#     import numpy as np\n",
    "#     import pandas as pd\n",
    "    \n",
    "#     raw_interaction = pd.read_csv(\"gs://raw-interaction-datasets/RAW_interactions.csv\")\n",
    "#     data = raw_interaction[['user_id','recipe_id','rating']]\n",
    "    \n",
    "#     # Cutting logic here ################################################################\n",
    "#     down_size = 10000\n",
    "#     data = data.head(down_size)\n",
    "\n",
    "    from pyspark.sql import SparkSession\n",
    "    \n",
    "    # Create a SparkSession\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Read Data from GCS\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    gcs_path = \"gs://raw-interaction-datasets/RAW_interactions.csv\"\n",
    "    raw_interaction = spark.read.csv(gcs_path, header=True, inferSchema=True)\n",
    "    data = raw_interaction[['user_id','recipe_id','rating']]\n",
    "    \n",
    "    print(\"Dataset path >>>>>>\" + dataset.path)\n",
    "    # data.to_csv(dataset.path, index=False)\n",
    "    data.write.csv(dataset.path, header=True, mode=\"overwrite\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4077cacb-eae3-4ba1-b1d0-a500c1930490",
   "metadata": {},
   "source": [
    "### Feature Engineering Component"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3b4125e5-53da-4ea9-bd55-d8c1587140af",
   "metadata": {},
   "source": [
    "################################################################\n",
    "\n",
    "Our Feature Engineer Component Here\n",
    "\n",
    "################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bd42fd-6814-4b39-95ed-61c7c311ff98",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10643d67-1740-4a58-a16c-e6a09f9653c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9\", \n",
    "    packages_to_install=[\"numpy\",\"pandas\",\"scikit-surprise\",\"fsspec\",\"gcsfs\",\"google-cloud-storage\"]\n",
    ")\n",
    "def training_model(\n",
    "    dataset: Input[Dataset],\n",
    "    training_status: Output[bool]\n",
    "):\n",
    "    import pickle\n",
    "    import pandas as pd\n",
    "    from google.cloud import storage\n",
    "\n",
    "    import surprise\n",
    "    from surprise import KNNBasic\n",
    "    from surprise import KNNWithMeans\n",
    "    from surprise import Dataset\n",
    "    from surprise import accuracy\n",
    "    from surprise import Reader\n",
    "    from surprise.model_selection import train_test_split\n",
    "    from surprise.model_selection import cross_validate\n",
    "    \n",
    "    print(\"Dataset path >>>>>>\" + dataset.path)\n",
    "    ratings_df = pd.read_csv(dataset.path)\n",
    "    \n",
    "    reader = Reader(rating_scale=(1,5))\n",
    "    data = Dataset.load_from_df(ratings_df, reader)  # assumes dataframe contains: user, item, ratings (in this order)\n",
    "    \n",
    "    # training\n",
    "    trainset, testset  = train_test_split(data, test_size=0.1)  # select 10% of rating events\n",
    "    algo = KNNBasic(k=40,sim_options={'name': 'msd', 'user_based': True }) # User-based CF\n",
    "    # algo = KNNBasic(k=40, sim_options={'name': 'cosine', 'user_based': True}) # ZeroDivisionError: float division WHY !!! \n",
    "    # algo = KNNBasic(k=40, sim_options={'name': 'pearson', 'user_based': True})\n",
    "    # algo = KNNBasic(k=40, sim_options={'name': 'pearson_baseline', 'user_based': True})\n",
    "    algo.fit(trainset)\n",
    "    \n",
    "    # Dump model pickle file\n",
    "    model_bytes = pickle.dumps(algo)\n",
    "    \n",
    "    # Upload the model pickle file to GCS\n",
    "    bucket_name = \"brave-watch-414204\"\n",
    "    pickle_file_name = \"knn_model.pkl\"\n",
    "    folder_path = \"Model\"\n",
    "    destination_blob_name = f'{folder_path}/{pickle_file_name}'\n",
    "    \n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    blob.upload_from_string(model_bytes)\n",
    "    training_status = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90caf1bb-6d12-453b-b733-ef44237900d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Construct Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02b016a6-d589-47b6-87ba-4a7f4eba4c91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=\"training_pipeline\",\n",
    "          pipeline_root=\"gs://brave-watch-414204\" + \"training_pipeline\")\n",
    "def basic_pipeline():\n",
    "    data_processing = get_data()\n",
    "    training = training_model(dataset = data_processing.outputs['dataset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bba53aa-7eb1-48dd-a880-b2abdf0049fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=basic_pipeline, \n",
    "    package_path=\"training_pipeline.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dc3eb03-97f4-43e5-8404-abe6951722d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "job = pipeline_jobs.PipelineJob(\n",
    "    display_name=\"training_pipeline\",\n",
    "    template_path=\"training_pipeline.yaml\",\n",
    "    enable_caching=False,\n",
    "    location=\"asia-southeast1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bbf8c3f-f6a0-4c72-b070-d69fdfed8d39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/564706877122/locations/asia-southeast1/pipelineJobs/training-pipeline-20240308030217\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/564706877122/locations/asia-southeast1/pipelineJobs/training-pipeline-20240308030217')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/asia-southeast1/pipelines/runs/training-pipeline-20240308030217?project=564706877122\n",
      "PipelineJob projects/564706877122/locations/asia-southeast1/pipelineJobs/training-pipeline-20240308030217 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/564706877122/locations/asia-southeast1/pipelineJobs/training-pipeline-20240308030217 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/564706877122/locations/asia-southeast1/pipelineJobs/training-pipeline-20240308030217 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/564706877122/locations/asia-southeast1/pipelineJobs/training-pipeline-20240308030217 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "job.run(sync=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a39387-078f-4547-98e6-b560d5bc3cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
